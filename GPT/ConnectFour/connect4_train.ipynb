{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wmasi\\anaconda3\\envs\\RL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from dataset import EpisodeDataset, collate_fn\n",
    "from model import Config, GPTModel\n",
    "from trainer import train_model, validate_model\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = {(i, j): i * 7 + j + 1 for i in range(6) for j in range(7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx['<pad>'] = 0  # Padding token\n",
    "\n",
    "vocab_size = 43\n",
    "block_size = 42 # Honestly this could probably be whatever\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, r'1\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent1 = pickle.load(f)\n",
    "with open(os.path.join(path, r'2\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent2 = pickle.load(f)\n",
    "with open(os.path.join(path, r'3\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent3 = pickle.load(f)\n",
    "with open(os.path.join(path, r'4\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent4 = pickle.load(f)\n",
    "with open(os.path.join(path, r'5\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent5 = pickle.load(f)\n",
    "with open(os.path.join(path, r'6\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent6 = pickle.load(f)\n",
    "with open(os.path.join(path, r'7\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent7 = pickle.load(f)\n",
    "with open(os.path.join(path, r'8\\training_data\\training_games_125000.pkl'), 'rb') as f:\n",
    "    agent8 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "valid_ratio = 0.1\n",
    "\n",
    "d1 = len(agent1)\n",
    "d2 = len(agent2)\n",
    "d3 = len(agent3)\n",
    "d4 = len(agent4)\n",
    "d5 = len(agent5)\n",
    "d6 = len(agent6)\n",
    "d7 = len(agent7)\n",
    "d8 = len(agent8)\n",
    "\n",
    "train1 = agent1[:int(train_ratio * d1)]\n",
    "valid1 = agent1[int(train_ratio * d1):int((train_ratio + valid_ratio) * d1) ]\n",
    "test1 = agent1[int((train_ratio + valid_ratio) * d1): ]\n",
    "\n",
    "train2 = agent2[:int(train_ratio * d2)]\n",
    "valid2 = agent2[int(train_ratio * d2):int((train_ratio + valid_ratio) * d2) ]\n",
    "test2 = agent2[int((train_ratio + valid_ratio) * d2): ]\n",
    "\n",
    "train3 = agent3[:int(train_ratio * d3)]\n",
    "valid3 = agent3[int(train_ratio * d3):int((train_ratio + valid_ratio) * d3)]\n",
    "test3 = agent3[int((train_ratio + valid_ratio) * d3):]\n",
    "\n",
    "train4 = agent4[:int(train_ratio * d4)]\n",
    "valid4 = agent4[int(train_ratio * d4):int((train_ratio + valid_ratio) * d4)]\n",
    "test4 = agent4[int((train_ratio + valid_ratio) * d4):]\n",
    "\n",
    "train5 = agent5[:int(train_ratio * d5)]\n",
    "valid5 = agent5[int(train_ratio * d5):int((train_ratio + valid_ratio) * d5)]\n",
    "test5 = agent5[int((train_ratio + valid_ratio) * d5):]\n",
    "\n",
    "train6 = agent6[:int(train_ratio * d6)]\n",
    "valid6 = agent6[int(train_ratio * d6):int((train_ratio + valid_ratio) * d6)]\n",
    "test6 = agent6[int((train_ratio + valid_ratio) * d6):]\n",
    "\n",
    "train7 = agent7[:int(train_ratio * d7)]\n",
    "valid7 = agent7[int(train_ratio * d7):int((train_ratio + valid_ratio) * d7)]\n",
    "test7 = agent7[int((train_ratio + valid_ratio) * d7):]\n",
    "\n",
    "train8 = agent8[:int(train_ratio * d8)]\n",
    "valid8 = agent8[int(train_ratio * d8):int((train_ratio + valid_ratio) * d8)]\n",
    "test8 = agent8[int((train_ratio + valid_ratio) * d8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000\n",
      "96000\n",
      "96008\n"
     ]
    }
   ],
   "source": [
    "train = train1 + train2 + train3 + train4 + train5 + train6 + train7 + train8\n",
    "valid = valid1 + valid2 + valid3 + valid4 + valid5 + valid6 + valid7 + valid8\n",
    "test = test1 + test2 + test3 + test4 + test5 + test6 + test7 + test8\n",
    "\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EpisodeDataset(train, token_to_idx)\n",
    "valid_dataset = EpisodeDataset(valid, token_to_idx)\n",
    "test_dataset = EpisodeDataset(test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(save_directory = None, epochs = 15):\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    config = Config(vocab_size, block_size, n_layer=num_layers, n_head=num_layers, n_embd=embed_size)\n",
    "    model = GPTModel(config)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "    \n",
    "    train_loader, valid_loader, model, scheduler, optimizer = accelerator.prepare(train_loader, valid_loader, model, scheduler, optimizer)\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    model_path = None\n",
    "    min_loss = 1e10\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        accelerator.print(f'Epoch {epoch}')\n",
    "\n",
    "        train_model(model, train_loader, optimizer, accelerator)\n",
    "        valid_loss = validate_model(model, valid_loader, accelerator)\n",
    "        scheduler.step()\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(f'Validation Loss: {valid_loss:.8f}')\n",
    "\n",
    "            model_save_path = f\"Model_{epoch+1}.pth\"\n",
    "            accelerator.save(accelerator.unwrap_model(model).state_dict(), model_save_path)\n",
    "\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss = valid_loss\n",
    "                model_path = model_save_path\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        shutil.copy(model_path, save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot find context for 'fork'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wmasi\\anaconda3\\envs\\RL\\lib\\site-packages\\accelerate\\launchers.py:201\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[1;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\wmasi\\anaconda3\\envs\\RL\\lib\\site-packages\\torch\\multiprocessing\\spawn.py:178\u001b[0m, in \u001b[0;36mstart_processes\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_processes\u001b[39m(\n\u001b[0;32m    176\u001b[0m     fn, args\u001b[38;5;241m=\u001b[39m(), nprocs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m ):\n\u001b[1;32m--> 178\u001b[0m     mp \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     error_queues \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    180\u001b[0m     processes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\wmasi\\anaconda3\\envs\\RL\\lib\\multiprocessing\\context.py:243\u001b[0m, in \u001b[0;36mDefaultContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_context\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wmasi\\anaconda3\\envs\\RL\\lib\\multiprocessing\\context.py:193\u001b[0m, in \u001b[0;36mBaseContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    191\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m _concrete_contexts[method]\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot find context for \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m method) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    194\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_check_available()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ctx\n",
      "\u001b[1;31mValueError\u001b[0m: cannot find context for 'fork'"
     ]
    }
   ],
   "source": [
    "notebook_launcher(train_main, (os.path.join(path, 'best_model'), 15), num_processes = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
