import torch
import numpy as np

import sys
sys.path.append('../')

from GPT.model import GPTModel, Config
from typing import Tuple, List, Optional, Dict

def return_idx(q_value_list: List, pos: Tuple[int]) -> Tuple[Optional[int], Optional[List[float]]]:

    """
    Return the index and Q-values where the specified position first appears.

    Parameters
    ----------
    q_value_list : List
        List containing state tuples and corresponding Q-values dictionaries.
    pos : Tuple[int]
        The position to match in the Q-values list.

    Returns
    -------
    Tuple[Optional[int], Optional[List[float]]]
        Index where the position was first found and the corresponding Q-values list; None if not found.
    """

    for i, value in enumerate(q_value_list):
        if value[0][0] == pos:
            q_vals = [value[0][1].get(action) for action in ['up', 'down', 'left', 'right']]
            return i, q_vals
    return None, None

def generate_probe_training_data(position: Tuple[int], dataset: List, q_list: List, probe_layer: int, 
                                 config: Config, token_to_idx: Dict, cutoff: int, model_load_path : str) -> Tuple[List[torch.Tensor], List[List[float]]]:
    
    """
    Generates embeddings and corresponding Q-value data for training the probe on a GPT model.

    Parameters
    ----------
    position : Tuple[int]
        Position after which the action is predicted in the network; used to slice the internal activation dimension.
    dataset : List
        The dataset generated by the RL agents.
    q_list : List
        The list of Q-values associated with the dataset.
    probe_layer : int
        The layer of the GPT model from which to extract embeddings.
    config : Config
        Configuration settings for the GPTModel.
    token_to_idx : Dict
        Mapping from state/action tokens to indices.
    cutoff : int
        Cutoff length after which we ignore the current embedding to avoid degenerate cases.
    model_load_path : str
        Path to load the Transformer Model

    Returns
    -------
    Tuple[List[torch.Tensor], List[List[float]]]
        Embeddings and their corresponding Q-values.
    """

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = GPTModel(config)
    state_dict = torch.load(model_load_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)

    embeddings = []
    q_values = []

    with torch.no_grad():
        for i, (X, _) in enumerate(dataset):
            if len(X) < cutoff:
                idx, q_vals = return_idx(q_list[i], position)
                if idx is not None and any(q_val != 0 for q_val in q_vals):
                    X_idx = [token_to_idx[token] for token in X]
                    X_idx = torch.tensor(X_idx, dtype=torch.long).to(device).unsqueeze(0)
                    embedding = model(X_idx, probe_layer)[:, idx, :]
                    embeddings.append(embedding.cpu().T)
                    q_values.append(q_vals)

    return embeddings, q_values

def get_embeddings_qvalues(positions: List[Tuple], data: List, q_data: List, layer: int, config : Config, token_to_idx : Dict, cutoff : int, model_load_path : str) -> Tuple[List, List]:

    """
    Retrieves embeddings and Q-values for specified positions and layers.

    Parameters
    ----------
    positions : List[Tuple]
        Positions to probe in the data.
    data : List
        Input dataset for embedding generation.
    q_data : List
        Q-values corresponding to the input dataset.
    layer : int
        Specific layer of the model to probe for embeddings.
    config : Config
        Configuration settings for the GPTModel.
    token_to_idx : Dict
        Mapping from state/action tokens to indices.
    cutoff : int
        Cutoff length after which we ignore the current embedding to avoid degenerate cases.
    model_load_path : str
        Path to load the Transformer Model

    Returns
    -------
    Tuple[List, List]
        Combined embeddings and Q-values for all positions.
    """

    embeddings = []
    q_values = []

    for pos in positions:
        curr_embed, curr_qvals = generate_probe_training_data(pos, data, q_data, layer, config, token_to_idx, cutoff, model_load_path)
        embeddings += curr_embed
        q_values += curr_qvals

    return embeddings, q_values

def min_max_normalization(data: List[float], min_val: Optional[float] = None, max_val: Optional[float] = None) -> Tuple[List[float], Optional[float], Optional[float]]:

    """
    Normalizes a list of values using the minimum and maximum values provided. If no min and max are provided,
    they are computed from the data.

    Parameters
    ----------
    data : List[float]
        Data to be normalized.
    min_val : Optional[float]
        Minimum value for normalization; computed if None.
    max_val : Optional[float]
        Maximum value for normalization; computed if None.

    Returns
    -------
    Tuple[List[float], Optional[float], Optional[float]]
        Normalized data and optionally the min and max values used for normalization.
    """

    data_array = np.array(data)
    compute_stats = min_val is None and max_val is None
    
    if compute_stats:
        min_val = np.min(data_array, axis=0)
        max_val = np.max(data_array, axis=0)

    normalized_data = (data_array - min_val) / (max_val - min_val)    
    normalized_data = normalized_data.tolist()
    
    if compute_stats:
        return normalized_data, min_val, max_val
    else:
        return normalized_data
