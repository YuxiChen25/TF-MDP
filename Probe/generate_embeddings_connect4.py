import torch
import numpy as np
import random
import pickle
import sys
sys.path.append('../')

from GPT.model import GPTModel, Config
from typing import Tuple, List, Optional, Dict

def return_idx(q_value_list: List) -> Tuple[Optional[int], Optional[List[float]]]:

    """
    Return a randomly sampled index and Q-value.

    Parameters
    ----------
    q_value_list : List
        List containing state tuples and corresponding Q-values dictionaries.
    Returns
    -------
    Tuple[Optional[int], Optional[List[float]]]
        Index where the position was first found and the corresponding Q-values list; None if not found.
    """

    pos = random.choice(range(len(q_value_list)))
    q_vals = [q_value_list[pos][0][1].get(action) for action in range(7)]
    return pos, q_vals

def generate_probe_training_data(num_samples: int, dataset: List, q_list: List, probe_layer: int, 
                                 config: Config, token_to_idx: Dict, model_load_path : str, data_path: str) -> Tuple[List[torch.Tensor], List[List[float]]]:
    
    """
    Generates embeddings and corresponding Q-value data for training the probe on a GPT model.

    Parameters
    ----------
    num_samples: int
        The number of samples to sample from each game. 
    dataset : List
        The dataset generated by the RL agents.
    q_list : List
        The list of Q-values associated with the dataset.
    probe_layer : int
        The layer of the GPT model from which to extract embeddings.
    config : Config
        Configuration settings for the GPTModel.
    token_to_idx : Dict
        Mapping from state/action tokens to indices.
    model_load_path : str
        Path to load the Transformer Model
    data_path: str
        Path to load preexisting samples from (if it exists)

    Returns
    -------
    Tuple[List[torch.Tensor], List[List[float]]]
        Embeddings and their corresponding Q-values.
    """

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = GPTModel(config)
    state_dict = torch.load(model_load_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)

    embeddings = []
    q_values = []

    sampled_data = []
    try:
        with open(data_path, 'rb') as f:
            sampled_data = pickle.load(f)
    except (FileNotFoundError):
        pass
    with torch.no_grad():
        for i, (X, _) in enumerate(dataset):
            for j in range(num_samples):
                if len(sampled_data) > 0:
                    idx, q_vals = sampled_data[i * num_samples + j]
                else:  
                    idx, q_vals = return_idx(q_list[i])
                if idx is not None and any(q_val != 0 for q_val in q_vals):
                    X_idx = [token_to_idx[token] for token in X]
                    X_idx = torch.tensor(X_idx, dtype=torch.long).to(device).unsqueeze(0)
                    embedding = model(X_idx, probe_layer)[:, idx, :]
                    embeddings.append(embedding.cpu().T)
                    q_values.append(q_vals)

    return embeddings, q_values

def get_embeddings_qvalues(num_samples: int, data: List, q_data: List, layer: int, config : Config, token_to_idx : Dict, model_load_path : str, data_path: str) -> Tuple[List, List]:

    """
    Retrieves embeddings and Q-values for specified positions and layers.

    Parameters
    ----------
    num_samples: int
        The number of samples to sample from each game. 
    data : List
        Input dataset for embedding generation.
    q_data : List
        Q-values corresponding to the input dataset.
    layer : int
        Specific layer of the model to probe for embeddings.
    config : Config
        Configuration settings for the GPTModel.
    token_to_idx : Dict
        Mapping from state/action tokens to indices.
    model_load_path : str
        Path to load the Transformer Model
    data_path: str
        Path to load preexisting samples from (if it exists)

    Returns
    -------
    Tuple[List, List]
        Combined embeddings and Q-values for all positions.
    """

    embeddings = []
    q_values = []

    curr_embed, curr_qvals = generate_probe_training_data(num_samples, data, q_data, layer, config, token_to_idx, model_load_path, data_path)
    embeddings += curr_embed
    q_values += curr_qvals

    return embeddings, q_values

def min_max_normalization(data: List[float], min_val: Optional[float] = None, max_val: Optional[float] = None) -> Tuple[List[float], Optional[float], Optional[float]]:

    """
    Normalizes a list of values using the minimum and maximum values provided. If no min and max are provided,
    they are computed from the data.

    Parameters
    ----------
    data : List[float]
        Data to be normalized.
    min_val : Optional[float]
        Minimum value for normalization; computed if None.
    max_val : Optional[float]
        Maximum value for normalization; computed if None.

    Returns
    -------
    Tuple[List[float], Optional[float], Optional[float]]
        Normalized data and optionally the min and max values used for normalization.
    """

    data_array = np.array(data)
    compute_stats = min_val is None and max_val is None
    
    if compute_stats:
        min_val = np.min(data_array, axis=0)
        max_val = np.max(data_array, axis=0)

    normalized_data = (data_array - min_val) / (max_val - min_val)    
    normalized_data = normalized_data.tolist()
    
    if compute_stats:
        return normalized_data, min_val, max_val
    else:
        return normalized_data
